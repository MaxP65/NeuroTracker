
    <!DOCTYPE html>
    <html lang="en">
    <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <title>[VG] Video Classification Benchmark</title>
        <script src="https://cdn.jsdelivr.net/npm/chart.js"></script>
        <script src="https://cdn.jsdelivr.net/npm/sortablejs@1.14.0/Sortable.min.js"></script>
        <style>
            body {
                font-family: Arial, sans-serif;
                margin: 0;
                padding: 20px;
                background-color: #f5f5f5;
            }
            .container {
                max-width: 1200px;
                margin: 0 auto;
                background-color: white;
                padding: 20px;
                border-radius: 8px;
                box-shadow: 0 2px 10px rgba(0,0,0,0.1);
            }
            h1 {
                text-align: center;
                color: #333;
            }
            .chart-container {
                margin: 30px 0;
                padding: 20px;
                background-color: white;
                border-radius: 8px;
                box-shadow: 0 1px 3px rgba(0,0,0,0.1);
            }
            canvas {
                max-height: 400px;
            }
            .controls {
                display: flex;
                justify-content: space-between;
                margin-bottom: 20px;
                flex-wrap: wrap;
            }
            select {
                padding: 8px;
                border-radius: 4px;
                border: 1px solid #ddd;
                min-width: 200px;
            }
            .tab {
                overflow: hidden;
                border: 1px solid #ccc;
                background-color: #f1f1f1;
                border-radius: 4px 4px 0 0;
            }
            .tab button {
                background-color: inherit;
                float: left;
                border: none;
                outline: none;
                cursor: pointer;
                padding: 14px 16px;
                transition: 0.3s;
                font-size: 16px;
            }
            .tab button:hover {
                background-color: #ddd;
            }
            .tab button.active {
                background-color: #fff;
                font-weight: bold;
            }
            .tabcontent {
                display: none;
                padding: 20px;
                border: 1px solid #ccc;
                border-top: none;
                border-radius: 0 0 4px 4px;
                animation: fadeEffect 1s;
            }
            @keyframes fadeEffect {
                from {opacity: 0;}
                to {opacity: 1;}
            }
            .tab-container {
                margin-bottom: 20px;
            }
            
    /* Performance Chart specific styles */
    .chart-container canvas {
        max-height: 400px;
    }
    
    /* Detailed Table specific styles */
    table {
        width: 100%;
        border-collapse: collapse;
        margin-top: 20px;
    }
    th, td {
        padding: 12px;
        text-align: left;
        border-bottom: 1px solid #ddd;
    }
    th {
        background-color: #f2f2f2;
        cursor: pointer;
    }
    tr:hover {
        background-color: #f5f5f5;
    }
    .metric-high {
        color: green;
    }
    .metric-low {
        color: red;
    }
    .model-name {
        position: relative;
        cursor: help;
    }
    .model-name:hover::after {
        content: attr(data-descr);
        position: absolute;
        left: 0;
        top: 100%;
        width: 300px;
        background-color: #fff;
        border: 1px solid #ddd;
        padding: 10px;
        border-radius: 4px;
        box-shadow: 0 2px 5px rgba(0,0,0,0.2);
        z-index: 100;
        font-size: 14px;
        line-height: 1.4;
    }
    .model-type {
        font-size: 0.8em;
        color: #666;
        display: block;
    }
    th.sort-asc::after {
        content: " ↓";
        font-size: 0.8em;
    }
    th.sort-desc::after {
        content: " ↑";
        font-size: 0.8em;
    }
    th.sortable {
        position: relative;
        padding-right: 20px;
        cursor: pointer;
    }
    th.sortable:hover {
        background-color: #e6e6e6;
    }
    
        </style>
    </head>
    <body>
        <div class="container">
            <h1>Video Classification Benchmark</h1>

            <div class="tab-container">
                <div class="tab">
                    
                    <button class="tablinks" onclick="openAttackTab(event, 'ifgsm-target-4')">ifgsm-target</button>
                    
                    <button class="tablinks" onclick="openAttackTab(event, 'ifgsm-untarget-4')">ifgsm-untarget</button>
                    
                    <button class="tablinks" onclick="openAttackTab(event, 'stylefool-untarget-1')">stylefool-untarget</button>
                    
                </div>

                
                <div id="ifgsm-target-4" class="tabcontent">
                    <h2>Iterative White Box Attack, target is least probable class in clean video, 1000 videos, eps 10/255</h2>

                    
    <div class="controls">
        <div>
            <label for="sort-metric-ifgsm-target-4">Sort by:</label>
            <select id="sort-metric-ifgsm-target-4" onchange="updateCharts('ifgsm-target-4')">
                <option value="mean_time_ms">Mean Time (ms)</option>
                <option value="mean_iterations">Mean Iterations</option>
                <option value="accuracy">Accuracy</option>
                <option value="avg_psnr">Average PSNR</option>
                <option value="avg_ssim">Average SSIM</option>
                <option value="avg_mse">Average MSE</option>
                <option value="attack_success_rate">Attack Success Rate</option>
            </select>
        </div>
        <div>
            <label for="sort-order-ifgsm-target-4">Order:</label>
            <select id="sort-order-ifgsm-target-4" onchange="updateCharts('ifgsm-target-4')">
                <option value="asc">Ascending</option>
                <option value="desc">Descending</option>
            </select>
        </div>
    </div>

    <div class="chart-container">
        <h3>Performance Metrics Comparison</h3>
        <canvas id="metricsChart-ifgsm-target-4"></canvas>
    </div>
    
    <div class="chart-container">
        <h3>Detailed Metrics</h3>
        <table id="metricsTable-ifgsm-target-4">
            <thead>
                <tr>
                    <th class="sortable" onclick="sortTable('ifgsm-target-4', 0)">Model</th>
                    <th class="sortable" onclick="sortTable('ifgsm-target-4', 1)">Type</th>
                    <th class="sortable" onclick="sortTable('ifgsm-target-4', 2)">Accuracy</th>
                    <th class="sortable" onclick="sortTable('ifgsm-target-4', 3)">Mean Time (ms)</th>
                    <th class="sortable" onclick="sortTable('ifgsm-target-4', 4)">Mean Iterations</th>
                    <th class="sortable" onclick="sortTable('ifgsm-target-4', 5)">Avg PSNR</th>
                    <th class="sortable" onclick="sortTable('ifgsm-target-4', 6)">Avg SSIM</th>
                    <th class="sortable" onclick="sortTable('ifgsm-target-4', 7)">Avg MSE</th>
                    <th class="sortable" onclick="sortTable('ifgsm-target-4', 8)">Attack Success (%)</th>
                </tr>
            </thead>
            <tbody>
                
                <tr>
                    <td class="model-name" data-descr="TSN, 2016: Temporal Segment Network with sparse sampling of video segments">tsn
                        <span class="model-type">2D Convolutional</span>
                    </td>
                    <td>2D Convolutional</td>
                    <td>71.9</td>
                    <td>3.39</td>
                    <td>11.14</td>
                    <td class="metric-high">
                        38.14
                    </td>
                    <td class="metric-high">
                        0.916
                    </td>
                    <td class="metric-high">
                        10.68
                    </td>
                    <td>92.2</td>
                </tr>
                
                <tr>
                    <td class="model-name" data-descr="UniFormerV2, 2023: Unified transformer with spatial-temporal aggregation, pre-trained on Kinetics-710">uniformerv2
                        <span class="model-type">Transformer</span>
                    </td>
                    <td>Transformer</td>
                    <td>85.2</td>
                    <td>5.17</td>
                    <td>12.80</td>
                    <td class="metric-high">
                        37.67
                    </td>
                    <td class="metric-high">
                        0.914
                    </td>
                    <td class="metric-high">
                        12.04
                    </td>
                    <td>95.8</td>
                </tr>
                
                <tr>
                    <td class="model-name" data-descr="VideoMAE, 2022: Original masked autoencoder for self-supervised video pre-training on Kinetics-400">videomae
                        <span class="model-type">Transformer</span>
                    </td>
                    <td>Transformer</td>
                    <td>80.0</td>
                    <td>3.72</td>
                    <td>5.11</td>
                    <td class="metric-high">
                        41.08
                    </td>
                    <td class="metric-high">
                        0.960
                    </td>
                    <td class="metric-high">
                        5.20
                    </td>
                    <td>100.0</td>
                </tr>
                
                <tr>
                    <td class="model-name" data-descr="VideoMAEv2, 2023: Masked autoencoder for self-supervised video representation learning, pre-trained on Kinetics-710">videomaev2
                        <span class="model-type">Transformer</span>
                    </td>
                    <td>Transformer</td>
                    <td>84.0</td>
                    <td>5.32</td>
                    <td>7.34</td>
                    <td class="metric-high">
                        39.67
                    </td>
                    <td class="metric-high">
                        0.947
                    </td>
                    <td class="metric-high">
                        7.23
                    </td>
                    <td>100.0</td>
                </tr>
                
                <tr>
                    <td class="model-name" data-descr="UniFormer, 2022: Unified transformer combining convolution and self-attention, pre-trained on ImageNet">uniformer
                        <span class="model-type">Transformer</span>
                    </td>
                    <td>Transformer</td>
                    <td>83.4</td>
                    <td>5.23</td>
                    <td>9.79</td>
                    <td class="metric-high">
                        38.62
                    </td>
                    <td class="metric-high">
                        0.925
                    </td>
                    <td class="metric-high">
                        9.26
                    </td>
                    <td>100.0</td>
                </tr>
                
                <tr>
                    <td class="model-name" data-descr="C2D, 2017: Basic 2D CNN baseline extended for video with temporal pooling">c2d
                        <span class="model-type">2D Convolutional</span>
                    </td>
                    <td>2D Convolutional</td>
                    <td>71.2</td>
                    <td>4.71</td>
                    <td>10.48</td>
                    <td class="metric-high">
                        38.39
                    </td>
                    <td class="metric-high">
                        0.923
                    </td>
                    <td class="metric-high">
                        10.06
                    </td>
                    <td>99.9</td>
                </tr>
                
                <tr>
                    <td class="model-name" data-descr="I3D-NL, 2018: Inflated 3D ConvNet with non-local attention blocks">i3dnonlocal
                        <span class="model-type">3D Convolutional</span>
                    </td>
                    <td>3D Convolutional</td>
                    <td>76.4</td>
                    <td>5.69</td>
                    <td>13.92</td>
                    <td class="metric-high">
                        37.53
                    </td>
                    <td class="metric-high">
                        0.911
                    </td>
                    <td class="metric-high">
                        11.95
                    </td>
                    <td>98.8</td>
                </tr>
                
                <tr>
                    <td class="model-name" data-descr="MViTv2, 2022: Improved multiscale vision transformer with hierarchical attention for video recognition">mvitv2
                        <span class="model-type">Transformer</span>
                    </td>
                    <td>Transformer</td>
                    <td>81.2</td>
                    <td>5.62</td>
                    <td>9.26</td>
                    <td class="metric-high">
                        38.89
                    </td>
                    <td class="metric-high">
                        0.937
                    </td>
                    <td class="metric-high">
                        8.65
                    </td>
                    <td>100.0</td>
                </tr>
                
                <tr>
                    <td class="model-name" data-descr="Video Swin Transformer, 2022: Hierarchical vision transformer with shifted windows for video recognition">videoswin
                        <span class="model-type">Transformer</span>
                    </td>
                    <td>Transformer</td>
                    <td>82.7</td>
                    <td>14.64</td>
                    <td>17.66</td>
                    <td class="metric-high">
                        36.14
                    </td>
                    <td class="metric-low">
                        0.878
                    </td>
                    <td class="metric-high">
                        16.20
                    </td>
                    <td>99.0</td>
                </tr>
                
                <tr>
                    <td class="model-name" data-descr="SlowFast, 2019: Dual-path network with slow (spatial) and fast (temporal) pathways">slowfast
                        <span class="model-type">Convolutional</span>
                    </td>
                    <td>Convolutional</td>
                    <td>78.8</td>
                    <td>9.47</td>
                    <td>19.81</td>
                    <td class="metric-high">
                        35.85
                    </td>
                    <td class="metric-high">
                        0.915
                    </td>
                    <td class="metric-high">
                        17.14
                    </td>
                    <td>42.2</td>
                </tr>
                
                <tr>
                    <td class="model-name" data-descr="I3D, 2017: Standard inflated 3D ConvNet using ImageNet pretrained 2D weights">i3d
                        <span class="model-type">3D Convolutional</span>
                    </td>
                    <td>3D Convolutional</td>
                    <td>74.5</td>
                    <td>5.65</td>
                    <td>14.64</td>
                    <td class="metric-high">
                        37.36
                    </td>
                    <td class="metric-high">
                        0.912
                    </td>
                    <td class="metric-high">
                        12.77
                    </td>
                    <td>97.0</td>
                </tr>
                
                <tr>
                    <td class="model-name" data-descr="R(2+1)D, 2018: 3D CNN variant with (2+1)D spatio-temporal convolutions">r2plus1d
                        <span class="model-type">3D Convolutional</span>
                    </td>
                    <td>3D Convolutional</td>
                    <td>74.5</td>
                    <td>6.88</td>
                    <td>9.68</td>
                    <td class="metric-high">
                        38.60
                    </td>
                    <td class="metric-high">
                        0.929
                    </td>
                    <td class="metric-high">
                        9.40
                    </td>
                    <td>100.0</td>
                </tr>
                
                <tr>
                    <td class="model-name" data-descr="TANet, 2020: Temporal aggregation network with attention for action recognition">tanet
                        <span class="model-type">Convolutional</span>
                    </td>
                    <td>Convolutional</td>
                    <td>72.3</td>
                    <td>7.85</td>
                    <td>12.20</td>
                    <td class="metric-high">
                        37.67
                    </td>
                    <td class="metric-high">
                        0.913
                    </td>
                    <td class="metric-high">
                        11.71
                    </td>
                    <td>99.8</td>
                </tr>
                
                <tr>
                    <td class="model-name" data-descr="SlowOnly, 2019: Simplified SlowFast without fast pathway, using non-local attention">slowonly
                        <span class="model-type">Convolutional</span>
                    </td>
                    <td>Convolutional</td>
                    <td>77.8</td>
                    <td>9.35</td>
                    <td>9.84</td>
                    <td class="metric-high">
                        38.66
                    </td>
                    <td class="metric-high">
                        0.927
                    </td>
                    <td class="metric-high">
                        9.36
                    </td>
                    <td>99.8</td>
                </tr>
                
                <tr>
                    <td class="model-name" data-descr="X3D, 2020: EfficientNet-like scaling of 3D CNNs for video recognition">x3d
                        <span class="model-type">Convolutional</span>
                    </td>
                    <td>Convolutional</td>
                    <td>75.9</td>
                    <td>15.42</td>
                    <td>14.06</td>
                    <td class="metric-high">
                        37.36
                    </td>
                    <td class="metric-high">
                        0.908
                    </td>
                    <td class="metric-high">
                        12.67
                    </td>
                    <td>98.6</td>
                </tr>
                
            </tbody>
        </table>
    </div>
    
                </div>
                
                <div id="ifgsm-untarget-4" class="tabcontent">
                    <h2>Iterative White Box Attack, target any class but gt, 1000 videos, eps 10/255</h2>

                    
    <div class="controls">
        <div>
            <label for="sort-metric-ifgsm-untarget-4">Sort by:</label>
            <select id="sort-metric-ifgsm-untarget-4" onchange="updateCharts('ifgsm-untarget-4')">
                <option value="mean_time_ms">Mean Time (ms)</option>
                <option value="mean_iterations">Mean Iterations</option>
                <option value="accuracy">Accuracy</option>
                <option value="avg_psnr">Average PSNR</option>
                <option value="avg_ssim">Average SSIM</option>
                <option value="avg_mse">Average MSE</option>
                <option value="attack_success_rate">Attack Success Rate</option>
            </select>
        </div>
        <div>
            <label for="sort-order-ifgsm-untarget-4">Order:</label>
            <select id="sort-order-ifgsm-untarget-4" onchange="updateCharts('ifgsm-untarget-4')">
                <option value="asc">Ascending</option>
                <option value="desc">Descending</option>
            </select>
        </div>
    </div>

    <div class="chart-container">
        <h3>Performance Metrics Comparison</h3>
        <canvas id="metricsChart-ifgsm-untarget-4"></canvas>
    </div>
    
    <div class="chart-container">
        <h3>Detailed Metrics</h3>
        <table id="metricsTable-ifgsm-untarget-4">
            <thead>
                <tr>
                    <th class="sortable" onclick="sortTable('ifgsm-untarget-4', 0)">Model</th>
                    <th class="sortable" onclick="sortTable('ifgsm-untarget-4', 1)">Type</th>
                    <th class="sortable" onclick="sortTable('ifgsm-untarget-4', 2)">Accuracy</th>
                    <th class="sortable" onclick="sortTable('ifgsm-untarget-4', 3)">Mean Time (ms)</th>
                    <th class="sortable" onclick="sortTable('ifgsm-untarget-4', 4)">Mean Iterations</th>
                    <th class="sortable" onclick="sortTable('ifgsm-untarget-4', 5)">Avg PSNR</th>
                    <th class="sortable" onclick="sortTable('ifgsm-untarget-4', 6)">Avg SSIM</th>
                    <th class="sortable" onclick="sortTable('ifgsm-untarget-4', 7)">Avg MSE</th>
                    <th class="sortable" onclick="sortTable('ifgsm-untarget-4', 8)">Attack Success (%)</th>
                </tr>
            </thead>
            <tbody>
                
                <tr>
                    <td class="model-name" data-descr="UniFormerV2, 2023: Unified transformer with spatial-temporal aggregation, pre-trained on Kinetics-710">uniformerv2
                        <span class="model-type">Transformer</span>
                    </td>
                    <td>Transformer</td>
                    <td>85.2</td>
                    <td>0.76</td>
                    <td>1.69</td>
                    <td class="metric-high">
                        49.78
                    </td>
                    <td class="metric-high">
                        0.993
                    </td>
                    <td class="metric-high">
                        0.72
                    </td>
                    <td>99.3</td>
                </tr>
                
                <tr>
                    <td class="model-name" data-descr="VideoMAE, 2022: Original masked autoencoder for self-supervised video pre-training on Kinetics-400">videomae
                        <span class="model-type">Transformer</span>
                    </td>
                    <td>Transformer</td>
                    <td>80.0</td>
                    <td>1.49</td>
                    <td>1.92</td>
                    <td class="metric-high">
                        49.77
                    </td>
                    <td class="metric-high">
                        0.993
                    </td>
                    <td class="metric-high">
                        0.72
                    </td>
                    <td>99.4</td>
                </tr>
                
                <tr>
                    <td class="model-name" data-descr="I3D-NL, 2018: Inflated 3D ConvNet with non-local attention blocks">i3dnonlocal
                        <span class="model-type">3D Convolutional</span>
                    </td>
                    <td>3D Convolutional</td>
                    <td>76.4</td>
                    <td>0.68</td>
                    <td>1.60</td>
                    <td class="metric-high">
                        49.92
                    </td>
                    <td class="metric-high">
                        0.993
                    </td>
                    <td class="metric-high">
                        0.70
                    </td>
                    <td>98.6</td>
                </tr>
                
                <tr>
                    <td class="model-name" data-descr="VideoMAEv2, 2023: Masked autoencoder for self-supervised video representation learning, pre-trained on Kinetics-710">videomaev2
                        <span class="model-type">Transformer</span>
                    </td>
                    <td>Transformer</td>
                    <td>84.0</td>
                    <td>1.87</td>
                    <td>2.46</td>
                    <td class="metric-high">
                        49.81
                    </td>
                    <td class="metric-high">
                        0.993
                    </td>
                    <td class="metric-high">
                        0.71
                    </td>
                    <td>88.3</td>
                </tr>
                
                <tr>
                    <td class="model-name" data-descr="I3D, 2017: Standard inflated 3D ConvNet using ImageNet pretrained 2D weights">i3d
                        <span class="model-type">3D Convolutional</span>
                    </td>
                    <td>3D Convolutional</td>
                    <td>74.5</td>
                    <td>0.68</td>
                    <td>1.66</td>
                    <td class="metric-high">
                        49.94
                    </td>
                    <td class="metric-high">
                        0.993
                    </td>
                    <td class="metric-high">
                        0.70
                    </td>
                    <td>96.7</td>
                </tr>
                
                <tr>
                    <td class="model-name" data-descr="C2D, 2017: Basic 2D CNN baseline extended for video with temporal pooling">c2d
                        <span class="model-type">2D Convolutional</span>
                    </td>
                    <td>2D Convolutional</td>
                    <td>71.2</td>
                    <td>0.84</td>
                    <td>1.74</td>
                    <td class="metric-high">
                        50.12
                    </td>
                    <td class="metric-high">
                        0.993
                    </td>
                    <td class="metric-high">
                        0.67
                    </td>
                    <td>99.3</td>
                </tr>
                
                <tr>
                    <td class="model-name" data-descr="R(2+1)D, 2018: 3D CNN variant with (2+1)D spatio-temporal convolutions">r2plus1d
                        <span class="model-type">3D Convolutional</span>
                    </td>
                    <td>3D Convolutional</td>
                    <td>74.5</td>
                    <td>1.13</td>
                    <td>1.46</td>
                    <td class="metric-high">
                        49.55
                    </td>
                    <td class="metric-high">
                        0.992
                    </td>
                    <td class="metric-high">
                        0.76
                    </td>
                    <td>99.8</td>
                </tr>
                
                <tr>
                    <td class="model-name" data-descr="X3D, 2020: EfficientNet-like scaling of 3D CNNs for video recognition">x3d
                        <span class="model-type">Convolutional</span>
                    </td>
                    <td>Convolutional</td>
                    <td>75.9</td>
                    <td>1.89</td>
                    <td>1.55</td>
                    <td class="metric-high">
                        49.76
                    </td>
                    <td class="metric-high">
                        0.992
                    </td>
                    <td class="metric-high">
                        0.73
                    </td>
                    <td>98.9</td>
                </tr>
                
                <tr>
                    <td class="model-name" data-descr="TSN, 2016: Temporal Segment Network with sparse sampling of video segments">tsn
                        <span class="model-type">2D Convolutional</span>
                    </td>
                    <td>2D Convolutional</td>
                    <td>71.9</td>
                    <td>0.64</td>
                    <td>1.88</td>
                    <td class="metric-high">
                        49.91
                    </td>
                    <td class="metric-high">
                        0.993
                    </td>
                    <td class="metric-high">
                        0.70
                    </td>
                    <td>95.1</td>
                </tr>
                
                <tr>
                    <td class="model-name" data-descr="SlowFast, 2019: Dual-path network with slow (spatial) and fast (temporal) pathways">slowfast
                        <span class="model-type">Convolutional</span>
                    </td>
                    <td>Convolutional</td>
                    <td>78.8</td>
                    <td>0.97</td>
                    <td>1.90</td>
                    <td class="metric-high">
                        49.68
                    </td>
                    <td class="metric-high">
                        0.994
                    </td>
                    <td class="metric-high">
                        0.74
                    </td>
                    <td>92.9</td>
                </tr>
                
                <tr>
                    <td class="model-name" data-descr="Video Swin Transformer, 2022: Hierarchical vision transformer with shifted windows for video recognition">videoswin
                        <span class="model-type">Transformer</span>
                    </td>
                    <td>Transformer</td>
                    <td>82.7</td>
                    <td>1.52</td>
                    <td>1.64</td>
                    <td class="metric-high">
                        49.85
                    </td>
                    <td class="metric-high">
                        0.992
                    </td>
                    <td class="metric-high">
                        0.71
                    </td>
                    <td>99.9</td>
                </tr>
                
                <tr>
                    <td class="model-name" data-descr="MViTv2, 2022: Improved multiscale vision transformer with hierarchical attention for video recognition">mvitv2
                        <span class="model-type">Transformer</span>
                    </td>
                    <td>Transformer</td>
                    <td>81.2</td>
                    <td>1.74</td>
                    <td>2.77</td>
                    <td class="metric-high">
                        50.01
                    </td>
                    <td class="metric-high">
                        0.994
                    </td>
                    <td class="metric-high">
                        0.65
                    </td>
                    <td>81.7</td>
                </tr>
                
                <tr>
                    <td class="model-name" data-descr="UniFormer, 2022: Unified transformer combining convolution and self-attention, pre-trained on ImageNet">uniformer
                        <span class="model-type">Transformer</span>
                    </td>
                    <td>Transformer</td>
                    <td>83.4</td>
                    <td>1.41</td>
                    <td>2.64</td>
                    <td class="metric-high">
                        50.00
                    </td>
                    <td class="metric-high">
                        0.993
                    </td>
                    <td class="metric-high">
                        0.65
                    </td>
                    <td>90.2</td>
                </tr>
                
                <tr>
                    <td class="model-name" data-descr="TANet, 2020: Temporal aggregation network with attention for action recognition">tanet
                        <span class="model-type">Convolutional</span>
                    </td>
                    <td>Convolutional</td>
                    <td>72.3</td>
                    <td>1.27</td>
                    <td>1.76</td>
                    <td class="metric-high">
                        49.92
                    </td>
                    <td class="metric-high">
                        0.993
                    </td>
                    <td class="metric-high">
                        0.70
                    </td>
                    <td>98.6</td>
                </tr>
                
                <tr>
                    <td class="model-name" data-descr="SlowOnly, 2019: Simplified SlowFast without fast pathway, using non-local attention">slowonly
                        <span class="model-type">Convolutional</span>
                    </td>
                    <td>Convolutional</td>
                    <td>77.8</td>
                    <td>1.89</td>
                    <td>1.80</td>
                    <td class="metric-high">
                        50.14
                    </td>
                    <td class="metric-high">
                        0.993
                    </td>
                    <td class="metric-high">
                        0.66
                    </td>
                    <td>99.5</td>
                </tr>
                
            </tbody>
        </table>
    </div>
    
                </div>
                
                <div id="stylefool-untarget-1" class="tabcontent">
                    <h2>Iterative Black Box Attack, target any class but gt, 250 videos, eps 90/255</h2>

                    
    <div class="controls">
        <div>
            <label for="sort-metric-stylefool-untarget-1">Sort by:</label>
            <select id="sort-metric-stylefool-untarget-1" onchange="updateCharts('stylefool-untarget-1')">
                <option value="mean_time_ms">Mean Time (ms)</option>
                <option value="mean_iterations">Mean Iterations</option>
                <option value="accuracy">Accuracy</option>
                <option value="avg_psnr">Average PSNR</option>
                <option value="avg_ssim">Average SSIM</option>
                <option value="avg_mse">Average MSE</option>
                <option value="attack_success_rate">Attack Success Rate</option>
            </select>
        </div>
        <div>
            <label for="sort-order-stylefool-untarget-1">Order:</label>
            <select id="sort-order-stylefool-untarget-1" onchange="updateCharts('stylefool-untarget-1')">
                <option value="asc">Ascending</option>
                <option value="desc">Descending</option>
            </select>
        </div>
    </div>

    <div class="chart-container">
        <h3>Performance Metrics Comparison</h3>
        <canvas id="metricsChart-stylefool-untarget-1"></canvas>
    </div>
    
    <div class="chart-container">
        <h3>Detailed Metrics</h3>
        <table id="metricsTable-stylefool-untarget-1">
            <thead>
                <tr>
                    <th class="sortable" onclick="sortTable('stylefool-untarget-1', 0)">Model</th>
                    <th class="sortable" onclick="sortTable('stylefool-untarget-1', 1)">Type</th>
                    <th class="sortable" onclick="sortTable('stylefool-untarget-1', 2)">Accuracy</th>
                    <th class="sortable" onclick="sortTable('stylefool-untarget-1', 3)">Mean Time (ms)</th>
                    <th class="sortable" onclick="sortTable('stylefool-untarget-1', 4)">Mean Iterations</th>
                    <th class="sortable" onclick="sortTable('stylefool-untarget-1', 5)">Avg PSNR</th>
                    <th class="sortable" onclick="sortTable('stylefool-untarget-1', 6)">Avg SSIM</th>
                    <th class="sortable" onclick="sortTable('stylefool-untarget-1', 7)">Avg MSE</th>
                    <th class="sortable" onclick="sortTable('stylefool-untarget-1', 8)">Attack Success (%)</th>
                </tr>
            </thead>
            <tbody>
                
                <tr>
                    <td class="model-name" data-descr="I3D, 2017: Standard inflated 3D ConvNet using ImageNet pretrained 2D weights">i3d
                        <span class="model-type">3D Convolutional</span>
                    </td>
                    <td>3D Convolutional</td>
                    <td>74.5</td>
                    <td>22.15</td>
                    <td>37.39</td>
                    <td class="metric-low">
                        17.23
                    </td>
                    <td class="metric-low">
                        0.177
                    </td>
                    <td class="metric-low">
                        1603.23
                    </td>
                    <td>68.3</td>
                </tr>
                
                <tr>
                    <td class="model-name" data-descr="I3D-NL, 2018: Inflated 3D ConvNet with non-local attention blocks">i3dnonlocal
                        <span class="model-type">3D Convolutional</span>
                    </td>
                    <td>3D Convolutional</td>
                    <td>76.4</td>
                    <td>28.62</td>
                    <td>43.25</td>
                    <td class="metric-low">
                        17.15
                    </td>
                    <td class="metric-low">
                        0.173
                    </td>
                    <td class="metric-low">
                        1626.81
                    </td>
                    <td>61.0</td>
                </tr>
                
                <tr>
                    <td class="model-name" data-descr="SlowFast, 2019: Dual-path network with slow (spatial) and fast (temporal) pathways">slowfast
                        <span class="model-type">Convolutional</span>
                    </td>
                    <td>Convolutional</td>
                    <td>78.8</td>
                    <td>29.28</td>
                    <td>36.21</td>
                    <td class="metric-low">
                        17.62
                    </td>
                    <td class="metric-low">
                        0.188
                    </td>
                    <td class="metric-low">
                        1523.28
                    </td>
                    <td>68.3</td>
                </tr>
                
                <tr>
                    <td class="model-name" data-descr="C2D, 2017: Basic 2D CNN baseline extended for video with temporal pooling">c2d
                        <span class="model-type">2D Convolutional</span>
                    </td>
                    <td>2D Convolutional</td>
                    <td>71.2</td>
                    <td>45.77</td>
                    <td>48.87</td>
                    <td class="metric-low">
                        16.70
                    </td>
                    <td class="metric-low">
                        0.164
                    </td>
                    <td class="metric-low">
                        1752.96
                    </td>
                    <td>54.9</td>
                </tr>
                
                <tr>
                    <td class="model-name" data-descr="R(2+1)D, 2018: 3D CNN variant with (2+1)D spatio-temporal convolutions">r2plus1d
                        <span class="model-type">3D Convolutional</span>
                    </td>
                    <td>3D Convolutional</td>
                    <td>74.5</td>
                    <td>48.29</td>
                    <td>25.78</td>
                    <td class="metric-low">
                        18.52
                    </td>
                    <td class="metric-low">
                        0.209
                    </td>
                    <td class="metric-low">
                        1281.58
                    </td>
                    <td>78.0</td>
                </tr>
                
                <tr>
                    <td class="model-name" data-descr="UniFormer, 2022: Unified transformer combining convolution and self-attention, pre-trained on ImageNet">uniformer
                        <span class="model-type">Transformer</span>
                    </td>
                    <td>Transformer</td>
                    <td>83.4</td>
                    <td>87.52</td>
                    <td>89.87</td>
                    <td class="metric-low">
                        14.65
                    </td>
                    <td class="metric-low">
                        0.113
                    </td>
                    <td class="metric-low">
                        2352.68
                    </td>
                    <td>11.0</td>
                </tr>
                
                <tr>
                    <td class="model-name" data-descr="TSN, 2016: Temporal Segment Network with sparse sampling of video segments">tsn
                        <span class="model-type">2D Convolutional</span>
                    </td>
                    <td>2D Convolutional</td>
                    <td>71.9</td>
                    <td>62.83</td>
                    <td>38.50</td>
                    <td class="metric-low">
                        18.05
                    </td>
                    <td class="metric-low">
                        0.199
                    </td>
                    <td class="metric-low">
                        1459.40
                    </td>
                    <td>65.4</td>
                </tr>
                
                <tr>
                    <td class="model-name" data-descr="TANet, 2020: Temporal aggregation network with attention for action recognition">tanet
                        <span class="model-type">Convolutional</span>
                    </td>
                    <td>Convolutional</td>
                    <td>72.3</td>
                    <td>100.70</td>
                    <td>51.88</td>
                    <td class="metric-low">
                        16.56
                    </td>
                    <td class="metric-low">
                        0.159
                    </td>
                    <td class="metric-low">
                        1770.61
                    </td>
                    <td>51.2</td>
                </tr>
                
                <tr>
                    <td class="model-name" data-descr="MViTv2, 2022: Improved multiscale vision transformer with hierarchical attention for video recognition">mvitv2
                        <span class="model-type">Transformer</span>
                    </td>
                    <td>Transformer</td>
                    <td>81.2</td>
                    <td>125.08</td>
                    <td>87.57</td>
                    <td class="metric-low">
                        14.76
                    </td>
                    <td class="metric-low">
                        0.122
                    </td>
                    <td class="metric-low">
                        2323.81
                    </td>
                    <td>13.8</td>
                </tr>
                
                <tr>
                    <td class="model-name" data-descr="VideoMAEv2, 2023: Masked autoencoder for self-supervised video representation learning, pre-trained on Kinetics-710">videomaev2
                        <span class="model-type">Transformer</span>
                    </td>
                    <td>Transformer</td>
                    <td>84.0</td>
                    <td>201.35</td>
                    <td>90.23</td>
                    <td class="metric-low">
                        14.91
                    </td>
                    <td class="metric-low">
                        0.123
                    </td>
                    <td class="metric-low">
                        2304.81
                    </td>
                    <td>10.2</td>
                </tr>
                
                <tr>
                    <td class="model-name" data-descr="X3D, 2020: EfficientNet-like scaling of 3D CNNs for video recognition">x3d
                        <span class="model-type">Convolutional</span>
                    </td>
                    <td>Convolutional</td>
                    <td>75.9</td>
                    <td>174.83</td>
                    <td>50.34</td>
                    <td class="metric-low">
                        16.64
                    </td>
                    <td class="metric-low">
                        0.162
                    </td>
                    <td class="metric-low">
                        1757.15
                    </td>
                    <td>52.8</td>
                </tr>
                
                <tr>
                    <td class="model-name" data-descr="UniFormerV2, 2023: Unified transformer with spatial-temporal aggregation, pre-trained on Kinetics-710">uniformerv2
                        <span class="model-type">Transformer</span>
                    </td>
                    <td>Transformer</td>
                    <td>85.2</td>
                    <td>153.85</td>
                    <td>83.74</td>
                    <td class="metric-low">
                        14.72
                    </td>
                    <td class="metric-low">
                        0.117
                    </td>
                    <td class="metric-low">
                        2314.83
                    </td>
                    <td>18.3</td>
                </tr>
                
                <tr>
                    <td class="model-name" data-descr="Video Swin Transformer, 2022: Hierarchical vision transformer with shifted windows for video recognition">videoswin
                        <span class="model-type">Transformer</span>
                    </td>
                    <td>Transformer</td>
                    <td>82.7</td>
                    <td>217.97</td>
                    <td>60.99</td>
                    <td class="metric-low">
                        16.09
                    </td>
                    <td class="metric-low">
                        0.149
                    </td>
                    <td class="metric-low">
                        1937.04
                    </td>
                    <td>42.3</td>
                </tr>
                
                <tr>
                    <td class="model-name" data-descr="VideoMAE, 2022: Original masked autoencoder for self-supervised video pre-training on Kinetics-400">videomae
                        <span class="model-type">Transformer</span>
                    </td>
                    <td>Transformer</td>
                    <td>80.0</td>
                    <td>224.64</td>
                    <td>91.55</td>
                    <td class="metric-low">
                        14.72
                    </td>
                    <td class="metric-low">
                        0.117
                    </td>
                    <td class="metric-low">
                        2344.68
                    </td>
                    <td>8.9</td>
                </tr>
                
                <tr>
                    <td class="model-name" data-descr="SlowOnly, 2019: Simplified SlowFast without fast pathway, using non-local attention">slowonly
                        <span class="model-type">Convolutional</span>
                    </td>
                    <td>Convolutional</td>
                    <td>77.8</td>
                    <td>187.48</td>
                    <td>46.91</td>
                    <td class="metric-low">
                        17.14
                    </td>
                    <td class="metric-low">
                        0.177
                    </td>
                    <td class="metric-low">
                        1642.54
                    </td>
                    <td>56.1</td>
                </tr>
                
            </tbody>
        </table>
    </div>
    
                </div>
                
            </div>
        </div>

        <script>
            // Prepare data
            const attacksData = {
                
                'ifgsm-target-4': [{"accuracy": 71.9, "attack_success_rate": 92.1608040201005, "avg_mse": 10.682394473082189, "avg_psnr": 38.1419544478779, "avg_ssim": 0.9163445294764473, "description": "TSN, 2016: Temporal Segment Network with sparse sampling of video segments", "mean_iterations": 11.136683417085427, "mean_time_ms": 3.39095459990765, "name": "tsn", "total_time": 5630.826047897339, "type": "2D Convolutional"}, {"accuracy": 85.2, "attack_success_rate": 95.77889447236181, "avg_mse": 12.043036222011635, "avg_psnr": 37.66547242657367, "avg_ssim": 0.913844635661448, "description": "UniFormerV2, 2023: Unified transformer with spatial-temporal aggregation, pre-trained on Kinetics-710", "mean_iterations": 12.798994974874372, "mean_time_ms": 5.1720051159211735, "name": "uniformerv2", "total_time": 10384.132328510284, "type": "Transformer"}, {"accuracy": 80.0, "attack_success_rate": 100.0, "avg_mse": 5.198510633580389, "avg_psnr": 41.07725935771346, "avg_ssim": 0.9604059267837787, "description": "VideoMAE, 2022: Original masked autoencoder for self-supervised video pre-training on Kinetics-400", "mean_iterations": 5.110552763819095, "mean_time_ms": 3.719960353602117, "name": "videomae", "total_time": 13280.269335269928, "type": "Transformer"}, {"accuracy": 84.0, "attack_success_rate": 100.0, "avg_mse": 7.231650289729245, "avg_psnr": 39.66563409348765, "avg_ssim": 0.946944662605101, "description": "VideoMAEv2, 2023: Masked autoencoder for self-supervised video representation learning, pre-trained on Kinetics-710", "mean_iterations": 7.335678391959799, "mean_time_ms": 5.3226004959949895, "name": "videomaev2", "total_time": 15013.048518657684, "type": "Transformer"}, {"accuracy": 83.4, "attack_success_rate": 100.0, "avg_mse": 9.261064475805934, "avg_psnr": 38.61614417825205, "avg_ssim": 0.9250075375449706, "description": "UniFormer, 2022: Unified transformer combining convolution and self-attention, pre-trained on ImageNet", "mean_iterations": 9.791959798994975, "mean_time_ms": 5.2262881293368695, "name": "uniformer", "total_time": 15557.34789943695, "type": "Transformer"}, {"accuracy": 71.2, "attack_success_rate": 99.89949748743719, "avg_mse": 10.060846444216129, "avg_psnr": 38.39082207308993, "avg_ssim": 0.9231975550972039, "description": "C2D, 2017: Basic 2D CNN baseline extended for video with temporal pooling", "mean_iterations": 10.480402010050252, "mean_time_ms": 4.710788109314501, "name": "c2d", "total_time": 16065.653198242188, "type": "2D Convolutional"}, {"accuracy": 76.4, "attack_success_rate": 98.79396984924624, "avg_mse": 11.954870957158576, "avg_psnr": 37.533155392894635, "avg_ssim": 0.9112011553122458, "description": "I3D-NL, 2018: Inflated 3D ConvNet with non-local attention blocks", "mean_iterations": 13.922613065326631, "mean_time_ms": 5.69037485314374, "name": "i3dnonlocal", "total_time": 16796.38593149185, "type": "3D Convolutional"}, {"accuracy": 81.2, "attack_success_rate": 100.0, "avg_mse": 8.6545214087314, "avg_psnr": 38.89217544393669, "avg_ssim": 0.9372309464927593, "description": "MViTv2, 2022: Improved multiscale vision transformer with hierarchical attention for video recognition", "mean_iterations": 9.2643216080402, "mean_time_ms": 5.619664521193385, "name": "mvitv2", "total_time": 15726.252759456636, "type": "Transformer"}, {"accuracy": 82.7, "attack_success_rate": 98.99497487437185, "avg_mse": 16.19868076904463, "avg_psnr": 36.13623524071975, "avg_ssim": 0.8783187925433693, "description": "Video Swin Transformer, 2022: Hierarchical vision transformer with shifted windows for video recognition", "mean_iterations": 17.656281407035177, "mean_time_ms": 14.642981046168648, "name": "videoswin", "total_time": 24317.43902254105, "type": "Transformer"}, {"accuracy": 78.8, "attack_success_rate": 42.211055276381906, "avg_mse": 17.13713229347944, "avg_psnr": 35.85344019473216, "avg_ssim": 0.914709846828152, "description": "SlowFast, 2019: Dual-path network with slow (spatial) and fast (temporal) pathways", "mean_iterations": 19.81005025125628, "mean_time_ms": 9.46548248008268, "name": "slowfast", "total_time": 20352.90156507492, "type": "Convolutional"}, {"accuracy": 74.5, "attack_success_rate": 96.98492462311557, "avg_mse": 12.76759360432773, "avg_psnr": 37.35990902164112, "avg_ssim": 0.911503272595717, "description": "I3D, 2017: Standard inflated 3D ConvNet using ImageNet pretrained 2D weights", "mean_iterations": 14.640201005025126, "mean_time_ms": 5.654907175284534, "name": "i3d", "total_time": 16259.689973592758, "type": "3D Convolutional"}, {"accuracy": 74.5, "attack_success_rate": 100.0, "avg_mse": 9.399625244753146, "avg_psnr": 38.59968785035473, "avg_ssim": 0.9285202690300032, "description": "R(2+1)D, 2018: 3D CNN variant with (2+1)D spatio-temporal convolutions", "mean_iterations": 9.680402010050251, "mean_time_ms": 6.87963454663454, "name": "r2plus1d", "total_time": 17775.627327680588, "type": "3D Convolutional"}, {"accuracy": 72.3, "attack_success_rate": 99.79899497487436, "avg_mse": 11.71056459841077, "avg_psnr": 37.66649800411128, "avg_ssim": 0.91305301089757, "description": "TANet, 2020: Temporal aggregation network with attention for action recognition", "mean_iterations": 12.201005025125628, "mean_time_ms": 7.85469731278156, "name": "tanet", "total_time": 18617.38445687294, "type": "Convolutional"}, {"accuracy": 77.8, "attack_success_rate": 99.79899497487436, "avg_mse": 9.356393130355697, "avg_psnr": 38.65989582606025, "avg_ssim": 0.926820853264188, "description": "SlowOnly, 2019: Simplified SlowFast without fast pathway, using non-local attention", "mean_iterations": 9.840201005025126, "mean_time_ms": 9.352201542542808, "name": "slowonly", "total_time": 20187.39397072792, "type": "Convolutional"}, {"accuracy": 75.9, "attack_success_rate": 98.5929648241206, "avg_mse": 12.671403846378817, "avg_psnr": 37.355082071439895, "avg_ssim": 0.9078930800142301, "description": "X3D, 2020: EfficientNet-like scaling of 3D CNNs for video recognition", "mean_iterations": 14.06030150753769, "mean_time_ms": 15.417201760546046, "name": "x3d", "total_time": 26678.82766866684, "type": "Convolutional"}],
                
                'ifgsm-untarget-4': [{"accuracy": 85.2, "attack_success_rate": 99.2964824120603, "avg_mse": 0.7213906048635472, "avg_psnr": 49.77522011079441, "avg_ssim": 0.9926674623360586, "description": "UniFormerV2, 2023: Unified transformer with spatial-temporal aggregation, pre-trained on Kinetics-710", "mean_iterations": 1.692462311557789, "mean_time_ms": 0.7588167365471922, "name": "uniformerv2", "total_time": 6119.837169647217, "type": "Transformer"}, {"accuracy": 80.0, "attack_success_rate": 99.39698492462311, "avg_mse": 0.7187357833448864, "avg_psnr": 49.77324405130408, "avg_ssim": 0.9931137810647491, "description": "VideoMAE, 2022: Original masked autoencoder for self-supervised video pre-training on Kinetics-400", "mean_iterations": 1.9175879396984925, "mean_time_ms": 1.4934646961077973, "name": "videomae", "total_time": 11469.396336317062, "type": "Transformer"}, {"accuracy": 76.4, "attack_success_rate": 98.5929648241206, "avg_mse": 0.6997575125151414, "avg_psnr": 49.91581416780621, "avg_ssim": 0.9925124113800828, "description": "I3D-NL, 2018: Inflated 3D ConvNet with non-local attention blocks", "mean_iterations": 1.5979899497487438, "mean_time_ms": 0.6833187764613473, "name": "i3dnonlocal", "total_time": 11929.849318265917, "type": "3D Convolutional"}, {"accuracy": 84.0, "attack_success_rate": 88.34170854271358, "avg_mse": 0.7063843250506892, "avg_psnr": 49.806729568663734, "avg_ssim": 0.9930327768646284, "description": "VideoMAEv2, 2023: Masked autoencoder for self-supervised video representation learning, pre-trained on Kinetics-710", "mean_iterations": 2.456281407035176, "mean_time_ms": 1.868871666558424, "name": "videomaev2", "total_time": 11825.118223905563, "type": "Transformer"}, {"accuracy": 74.5, "attack_success_rate": 96.68341708542714, "avg_mse": 0.6955200898699976, "avg_psnr": 49.93529119938667, "avg_ssim": 0.9927127046328994, "description": "I3D, 2017: Standard inflated 3D ConvNet using ImageNet pretrained 2D weights", "mean_iterations": 1.6613065326633163, "mean_time_ms": 0.6825589664018333, "name": "i3d", "total_time": 11976.99990582466, "type": "3D Convolutional"}, {"accuracy": 71.2, "attack_success_rate": 99.2964824120603, "avg_mse": 0.6678379949863761, "avg_psnr": 50.119203320289785, "avg_ssim": 0.9929981639936336, "description": "C2D, 2017: Basic 2D CNN baseline extended for video with temporal pooling", "mean_iterations": 1.735678391959799, "mean_time_ms": 0.8437916801203436, "name": "c2d", "total_time": 12323.271506786346, "type": "2D Convolutional"}, {"accuracy": 74.5, "attack_success_rate": 99.79899497487436, "avg_mse": 0.7601416811134986, "avg_psnr": 49.550592448194905, "avg_ssim": 0.991814224706522, "description": "R(2+1)D, 2018: 3D CNN variant with (2+1)D spatio-temporal convolutions", "mean_iterations": 1.456281407035176, "mean_time_ms": 1.1300270679608062, "name": "r2plus1d", "total_time": 12690.111619472504, "type": "3D Convolutional"}, {"accuracy": 75.9, "attack_success_rate": 98.89447236180905, "avg_mse": 0.7284922072489038, "avg_psnr": 49.75589949621914, "avg_ssim": 0.9924113246940789, "description": "X3D, 2020: EfficientNet-like scaling of 3D CNNs for video recognition", "mean_iterations": 1.5527638190954771, "mean_time_ms": 1.8949286448895637, "name": "x3d", "total_time": 14073.238441228868, "type": "Convolutional"}, {"accuracy": 71.9, "attack_success_rate": 95.07537688442211, "avg_mse": 0.6981118119513248, "avg_psnr": 49.90696682497579, "avg_ssim": 0.9925046746080844, "description": "TSN, 2016: Temporal Segment Network with sparse sampling of video segments", "mean_iterations": 1.877386934673367, "mean_time_ms": 0.6406220366607359, "name": "tsn", "total_time": 2906.303254365921, "type": "2D Convolutional"}, {"accuracy": 78.8, "attack_success_rate": 92.8643216080402, "avg_mse": 0.7353904488570915, "avg_psnr": 49.683088987334955, "avg_ssim": 0.9935741147204243, "description": "SlowFast, 2019: Dual-path network with slow (spatial) and fast (temporal) pathways", "mean_iterations": 1.8954773869346733, "mean_time_ms": 0.9664726683841878, "name": "slowfast", "total_time": 12370.200655937197, "type": "Convolutional"}, {"accuracy": 82.7, "attack_success_rate": 99.89949748743719, "avg_mse": 0.7118437918075583, "avg_psnr": 49.84833170836626, "avg_ssim": 0.9924399163682548, "description": "Video Swin Transformer, 2022: Hierarchical vision transformer with shifted windows for video recognition", "mean_iterations": 1.636180904522613, "mean_time_ms": 1.521981359127179, "name": "videoswin", "total_time": 10586.446232318878, "type": "Transformer"}, {"accuracy": 81.2, "attack_success_rate": 81.70854271356785, "avg_mse": 0.6533375102637543, "avg_psnr": 50.00940964448779, "avg_ssim": 0.9936061101695105, "description": "MViTv2, 2022: Improved multiscale vision transformer with hierarchical attention for video recognition", "mean_iterations": 2.770854271356784, "mean_time_ms": 1.74293381820372, "name": "mvitv2", "total_time": 11808.63102698326, "type": "Transformer"}, {"accuracy": 83.4, "attack_success_rate": 90.15075376884421, "avg_mse": 0.6544492223876118, "avg_psnr": 50.00098810485057, "avg_ssim": 0.9932300457320933, "description": "UniFormer, 2022: Unified transformer combining convolution and self-attention, pre-trained on ImageNet", "mean_iterations": 2.639195979899497, "mean_time_ms": 1.4114345274977949, "name": "uniformer", "total_time": 11061.918256521223, "type": "Transformer"}, {"accuracy": 72.3, "attack_success_rate": 98.5929648241206, "avg_mse": 0.6991600038453274, "avg_psnr": 49.92335187769132, "avg_ssim": 0.9928430478873856, "description": "TANet, 2020: Temporal aggregation network with attention for action recognition", "mean_iterations": 1.757788944723618, "mean_time_ms": 1.2664492595135866, "name": "tanet", "total_time": 12571.729223251345, "type": "Convolutional"}, {"accuracy": 77.8, "attack_success_rate": 99.49748743718592, "avg_mse": 0.6649897484066868, "avg_psnr": 50.139893916216145, "avg_ssim": 0.9932430297450794, "description": "SlowOnly, 2019: Simplified SlowFast without fast pathway, using non-local attention", "mean_iterations": 1.7979899497487437, "mean_time_ms": 1.8851291356973312, "name": "slowonly", "total_time": 13068.870289802551, "type": "Convolutional"}],
                
                'stylefool-untarget-1': [{"accuracy": 74.5, "attack_success_rate": 68.29268292682927, "avg_mse": 1603.2262709164397, "avg_psnr": 17.228491659372555, "avg_ssim": 0.17678961859753728, "description": "I3D, 2017: Standard inflated 3D ConvNet using ImageNet pretrained 2D weights", "mean_iterations": 37.39024390243903, "mean_time_ms": 22.1455437749382, "name": "i3d", "total_time": 8527.915002346039, "type": "3D Convolutional"}, {"accuracy": 76.4, "attack_success_rate": 60.97560975609756, "avg_mse": 1626.8109039381056, "avg_psnr": 17.154972349525863, "avg_ssim": 0.1731864156588174, "description": "I3D-NL, 2018: Inflated 3D ConvNet with non-local attention blocks", "mean_iterations": 43.2479674796748, "mean_time_ms": 28.61837168050006, "name": "i3dnonlocal", "total_time": 10135.79863333702, "type": "3D Convolutional"}, {"accuracy": 78.8, "attack_success_rate": 68.29268292682927, "avg_mse": 1523.2789431069161, "avg_psnr": 17.62370290720028, "avg_ssim": 0.18796265325203476, "description": "SlowFast, 2019: Dual-path network with slow (spatial) and fast (temporal) pathways", "mean_iterations": 36.21138211382114, "mean_time_ms": 29.276792524306757, "name": "slowfast", "total_time": 10282.476969242096, "type": "Convolutional"}, {"accuracy": 71.2, "attack_success_rate": 54.87804878048781, "avg_mse": 1752.9584594715245, "avg_psnr": 16.698587644698208, "avg_ssim": 0.16382342037472475, "description": "C2D, 2017: Basic 2D CNN baseline extended for video with temporal pooling", "mean_iterations": 48.86585365853659, "mean_time_ms": 45.76931463315235, "name": "c2d", "total_time": 14257.793087720873, "type": "2D Convolutional"}, {"accuracy": 74.5, "attack_success_rate": 78.04878048780488, "avg_mse": 1281.5781889609743, "avg_psnr": 18.5187708473306, "avg_ssim": 0.2093657188831214, "description": "R(2+1)D, 2018: 3D CNN variant with (2+1)D spatio-temporal convolutions", "mean_iterations": 25.776422764227643, "mean_time_ms": 48.28597321936755, "name": "r2plus1d", "total_time": 14858.272216558456, "type": "3D Convolutional"}, {"accuracy": 83.4, "attack_success_rate": 10.975609756097562, "avg_mse": 2352.680947143357, "avg_psnr": 14.645007311957494, "avg_ssim": 0.11344622601646857, "description": "UniFormer, 2022: Unified transformer combining convolution and self-attention, pre-trained on ImageNet", "mean_iterations": 89.8739837398374, "mean_time_ms": 87.51526156673586, "name": "uniformer", "total_time": 24237.70437645912, "type": "Transformer"}, {"accuracy": 71.9, "attack_success_rate": 65.4471544715447, "avg_mse": 1459.403293960224, "avg_psnr": 18.054073411397663, "avg_ssim": 0.19926544144526986, "description": "TSN, 2016: Temporal Segment Network with sparse sampling of video segments", "mean_iterations": 38.5, "mean_time_ms": 62.831804620541206, "name": "tsn", "total_time": 16208.048182487488, "type": "2D Convolutional"}, {"accuracy": 72.3, "attack_success_rate": 51.21951219512195, "avg_mse": 1770.6055434485843, "avg_psnr": 16.560354114164355, "avg_ssim": 0.1593016826222245, "description": "TANet, 2020: Temporal aggregation network with attention for action recognition", "mean_iterations": 51.8780487804878, "mean_time_ms": 100.70018396823389, "name": "tanet", "total_time": 27702.75557827949, "type": "Convolutional"}, {"accuracy": 81.2, "attack_success_rate": 13.821138211382115, "avg_mse": 2323.8109968120034, "avg_psnr": 14.758957555734465, "avg_ssim": 0.1217500487366567, "description": "MViTv2, 2022: Improved multiscale vision transformer with hierarchical attention for video recognition", "mean_iterations": 87.57317073170732, "mean_time_ms": 125.08202592725677, "name": "mvitv2", "total_time": 33463.28730034828, "type": "Transformer"}, {"accuracy": 84.0, "attack_success_rate": 10.16260162601626, "avg_mse": 2304.814755727015, "avg_psnr": 14.910315186941126, "avg_ssim": 0.12251664949669169, "description": "VideoMAEv2, 2023: Masked autoencoder for self-supervised video representation learning, pre-trained on Kinetics-710", "mean_iterations": 90.22764227642276, "mean_time_ms": 201.35090276283947, "name": "videomaev2", "total_time": 52100.07508516312, "type": "Transformer"}, {"accuracy": 75.9, "attack_success_rate": 52.84552845528455, "avg_mse": 1757.1484097675543, "avg_psnr": 16.644903567454495, "avg_ssim": 0.16234194170802976, "description": "X3D, 2020: EfficientNet-like scaling of 3D CNNs for video recognition", "mean_iterations": 50.33739837398374, "mean_time_ms": 174.82831309287528, "name": "x3d", "total_time": 46130.00055503845, "type": "Convolutional"}, {"accuracy": 85.2, "attack_success_rate": 18.29268292682927, "avg_mse": 2314.832578240076, "avg_psnr": 14.724686246873523, "avg_ssim": 0.11677734814370856, "description": "UniFormerV2, 2023: Unified transformer with spatial-temporal aggregation, pre-trained on Kinetics-710", "mean_iterations": 83.73577235772358, "mean_time_ms": 153.8468048281786, "name": "uniformerv2", "total_time": 39269.54822731018, "type": "Transformer"}, {"accuracy": 82.7, "attack_success_rate": 42.27642276422765, "avg_mse": 1937.044374593322, "avg_psnr": 16.088438947229793, "avg_ssim": 0.14913988290058106, "description": "Video Swin Transformer, 2022: Hierarchical vision transformer with shifted windows for video recognition", "mean_iterations": 60.98780487804878, "mean_time_ms": 217.97201506781383, "name": "videoswin", "total_time": 56158.54801607132, "type": "Transformer"}, {"accuracy": 80.0, "attack_success_rate": 8.94308943089431, "avg_mse": 2344.680156925777, "avg_psnr": 14.717698493553852, "avg_ssim": 0.11659536454468566, "description": "VideoMAE, 2022: Original masked autoencoder for self-supervised video pre-training on Kinetics-400", "mean_iterations": 91.54878048780488, "mean_time_ms": 224.6423280927224, "name": "videomae", "total_time": 57809.16391038895, "type": "Transformer"}, {"accuracy": 77.8, "attack_success_rate": 56.09756097560976, "avg_mse": 1642.5360710793414, "avg_psnr": 17.13722600039747, "avg_ssim": 0.17662920531186727, "description": "SlowOnly, 2019: Simplified SlowFast without fast pathway, using non-local attention", "mean_iterations": 46.90650406504065, "mean_time_ms": 187.48250503947096, "name": "slowonly", "total_time": 49019.57434988022, "type": "Convolutional"}],
                
            };


            // Tab functionality
            function openAttackTab(evt, attackId) {
                // Hide all tabcontent
                const tabcontent = document.getElementsByClassName("tabcontent");
                for (let i = 0; i < tabcontent.length; i++) {
                    tabcontent[i].style.display = "none";
                }

                // Remove active class from all tablinks
                const tablinks = document.getElementsByClassName("tablinks");
                for (let i = 0; i < tablinks.length; i++) {
                    tablinks[i].className = tablinks[i].className.replace(" active", "");
                }

                // Show the current tab and add active class
                document.getElementById(attackId).style.display = "block";
                evt.currentTarget.className += " active";

                // Update charts for this tab if not already initialized
                if (!charts[attackId]) {
                    updateCharts(attackId);
                }
            }

            // Initialize first tab as active
            document.addEventListener('DOMContentLoaded', function() {
                const firstTab = document.querySelector('.tablinks');
                if (firstTab) {
                    firstTab.click();
                }
            });
            
            
    // Chart instances storage
    const charts = {};
    function updateCharts(attackId) {
        const sortMetric = document.getElementById(`sort-metric-${attackId}`).value;
        const sortOrder = document.getElementById(`sort-order-${attackId}`).value;
        const models = attacksData[attackId];

        // Sort models
        const sortedModels = [...models].sort((a, b) => {
            const valA = a[sortMetric] || 0;
            const valB = b[sortMetric] || 0;
            return sortOrder === 'asc' ? valA - valB : valB - valA;
        });

        // Update metrics chart
        updateMetricsChart(attackId, sortedModels, sortMetric);
    }

    function updateMetricsChart(attackId, sortedModels, sortMetric) {
        const ctx = document.getElementById(`metricsChart-${attackId}`).getContext('2d');

        const labels = sortedModels.map(m => m.name);

        // Determine which datasets to show based on sort metric
        let datasets = [];
        let yAxes = {};

        if (sortMetric === 'mean_time_ms' || sortMetric === 'mean_iterations') {
            // Show time and iterations when sorted by either
            datasets = [
                {
                    label: 'Mean Time (ms)',
                    data: sortedModels.map(m => m.mean_time_ms),
                    backgroundColor: 'rgba(54, 162, 235, 0.7)',
                    borderColor: 'rgba(54, 162, 235, 1)',
                    borderWidth: 1,
                    yAxisID: 'y'
                },
                {
                    label: 'Mean Iterations',
                    data: sortedModels.map(m => m.mean_iterations),
                    backgroundColor: 'rgba(255, 99, 132, 0.7)',
                    borderColor: 'rgba(255, 99, 132, 1)',
                    borderWidth: 1,
                    yAxisID: 'y1'
                }
            ];

            yAxes = {
                y: {
                    beginAtZero: true,
                    title: {
                        display: true,
                        text: 'Mean Time (ms)'
                    }
                },
                y1: {
                    type: 'linear',
                    display: true,
                    position: 'right',
                    beginAtZero: true,
                    title: {
                        display: true,
                        text: 'Mean Iterations'
                    },
                    grid: {
                        drawOnChartArea: false
                    }
                }
            };
        } else if (sortMetric === 'accuracy') {
            // Show accuracy and attack success rate when sorted by accuracy
            datasets = [
                {
                    label: 'Accuracy',
                    data: sortedModels.map(m => m.accuracy),
                    backgroundColor: 'rgba(54, 162, 235, 0.7)',
                    borderColor: 'rgba(54, 162, 235, 1)',
                    borderWidth: 1,
                    yAxisID: 'y'
                },
                {
                    label: 'Attack Success Rate',
                    data: sortedModels.map(m => m.attack_success_rate),
                    backgroundColor: 'rgba(255, 99, 132, 0.7)',
                    borderColor: 'rgba(255, 99, 132, 1)',
                    borderWidth: 1,
                    yAxisID: 'y1'
                }
            ];

            yAxes = {
                y: {
                    beginAtZero: true,
                    max: 100,
                    title: {
                        display: true,
                        text: 'Accuracy (%)'
                    }
                },
                y1: {
                    type: 'linear',
                    display: true,
                    position: 'right',
                    beginAtZero: true,
                    max: 100,
                    title: {
                        display: true,
                        text: 'Attack Success Rate (%)'
                    },
                    grid: {
                        drawOnChartArea: false
                    }
                }
            };
        } else {
            // Show quality metrics for other sort options
            datasets = [
                {
                    label: 'PSNR',
                    data: sortedModels.map(m => m.avg_psnr),
                    backgroundColor: 'rgba(54, 162, 235, 0.7)',
                    borderColor: 'rgba(54, 162, 235, 1)',
                    borderWidth: 1
                },
                {
                    label: 'SSIM',
                    data: sortedModels.map(m => m.avg_ssim),
                    backgroundColor: 'rgba(255, 99, 132, 0.7)',
                    borderColor: 'rgba(255, 99, 132, 1)',
                    borderWidth: 1,
                    yAxisID: 'y1'
                },
                {
                    label: 'MSE',
                    data: sortedModels.map(m => m.avg_mse),
                    backgroundColor: 'rgba(75, 192, 192, 0.7)',
                    borderColor: 'rgba(75, 192, 192, 1)',
                    borderWidth: 1,
                    yAxisID: 'y2'
                }
            ];

            yAxes = {
                y: {
                    beginAtZero: false,
                    title: {
                        display: true,
                        text: 'PSNR (dB)'
                    }
                },
                y1: {
                    type: 'linear',
                    display: true,
                    position: 'right',
                    beginAtZero: true,
                    max: 1,
                    title: {
                        display: true,
                        text: 'SSIM'
                    },
                    grid: {
                        drawOnChartArea: false
                    }
                },
                y2: {
                    type: 'linear',
                    display: false,
                    beginAtZero: true,
                    title: {
                        display: true,
                        text: 'MSE'
                    }
                }
            };
        }

        // Destroy existing chart if it exists
        if (charts[attackId]) {
            charts[attackId].destroy();
        }

        // Create new chart
        charts[attackId] = new Chart(ctx, {
            type: 'bar',
            data: {
                labels: labels,
                datasets: datasets
            },
            options: {
                responsive: true,
                scales: yAxes
            }
        });
    }
    
    function sortTable(attackId, columnIndex) {
        const table = document.getElementById(`metricsTable-${attackId}`);
        const tbody = table.querySelector('tbody');
        const rows = Array.from(tbody.querySelectorAll('tr'));
        const headers = table.querySelectorAll('th');
    
        // Remove all sort classes from headers
        headers.forEach(header => {
            header.classList.remove('sort-asc', 'sort-desc');
        });
    
        // Determine sort direction
        let sortDirection = 'asc';
        if (table.dataset.lastSorted === columnIndex.toString()) {
            sortDirection = table.dataset.lastSortDirection === 'asc' ? 'desc' : 'asc';
        }
        table.dataset.lastSortDirection = sortDirection;
        table.dataset.lastSorted = columnIndex.toString();
    
        // Add sort class to current header
        headers[columnIndex].classList.add(sortDirection === 'asc' ? 'sort-asc' : 'sort-desc');
    
        rows.sort((a, b) => {
            const aVal = a.cells[columnIndex].textContent;
            const bVal = b.cells[columnIndex].textContent;
    
            // Handle numeric values
            if (!isNaN(aVal) && !isNaN(bVal)) {
                return parseFloat(aVal) - parseFloat(bVal);
            }
    
            // Handle N/A values
            if (aVal === 'N/A') return 1;
            if (bVal === 'N/A') return -1;
    
            // Default string comparison
            return aVal.localeCompare(bVal);
        });
    
        // Reverse if descending
        if (sortDirection === 'desc') {
            rows.reverse();
        }
    
        // Rebuild table
        rows.forEach(row => tbody.appendChild(row));
    }
    
        </script>
    </body>
    </html>
    